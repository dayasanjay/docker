# Learning Objectives

- What is Docker?
- Containerization vs virtualization
- Advantages of Containerization
- Understanding Docker Terminologies
- Listing out important commands in Docker
- Run command options in Docker
- Downloading the image and creating container
- Stopping the container and removing the container
- Understanding detached mode and interactive mode
- Using environment variable
- Creating multi-container architecture using --link
- Implementing LAMP Architecture using Docker
- Creating CI-CD Environment using Docker
- Creating testing environment using Docker
- Installing Docker compose
- Examples of docker-compose file
- Understanding Docker volume
- Types of docker volume
- Creating customized docker images
- Understanding Keywords of Docker file
- Working with Docker file
- Version controlling on Docker file
- Cache Busting
- Working with Registry
- Understanding the default process of the container
- Changing the default process of the container
- Container Orchestration
- Docker swarm

# What is Docker?
Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications. By taking advantage of Dockerâ€™s methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production.

# Containerization vs virtualization
Virtualization -- Fixed hardware allocation.<br>
Containerization - No Fixed Hardware<br>
Process isolation ( Dependency on os is removed )<br>

# Advantages of Containerization
In comparison to the traditional virtualization functionalities of hypervisors, Docker containers eliminate the need for a separate guest operating system for every new virtual machine. Docker implements a high-level API to provide lightweight containers that run processes in isolation. A Docker container enables rapid deployment with minimum run-time requirements. It also ensures better management and simplified portability. This helps developers and operations team in rapid deployment of an application.

# Understanding Docker Terminologies
We should be comformatable with four terms:<br>

1) Docker Images:<br>
Combinations of binaries/libraries which are necessary for one software application.<br>
2) Docker Containers:<br>
When image is installed and in comes into running condition, it is called container.<br>
3) Docker Host:<br>
Machine on which docker is installed, is called as Docker host.<br>
4) Docker Client:<br>
Terminal used to run docker run commands ( Git bash )<br>

On linux machine, git bash will work like docker client.

# Listing out important commands in Docker

Docker Commands<br>
Working on Images :<br>
1) To download a docker image<br> 
   docker pull <image_name> 

2) To see the list of docker images<br> 
  docker image ls<br> 
  (or)<br> 
  docker images<br> 

3) To delete a docker image from docker host<br> 
  docker rmi <image_name/image_id><br> 

4) To upload a docker image into docker hub<br> 
   docker push <image_name><br> 

5) To tag an image<br> 
docker tag <image_name> ipaddress_of_local_registry:5000/image_name<br> 

6) To build an image from a customised container<br> 
  docker commit container_name/container_id>  <new_image_name><br> 

7) To create an image from docker file<br> 
   docker build -t  <new_image_name><br> 

8) To search for a docker image<br> 
   docker search <image_name><br> 

9)  To delete all images that are not attached to containers<br> 
   docker system prune -a<br> 

# Working on containers


10) To see the list of all running continers<br> 
   docker  container  ls<br> 

11) To see the list of running and stopped containers<br> 
    docker ps -a<br> 

12) To start a container<br> 
    docker start container_name/container_id<br> 

13) To stop a running container<br> 
    docker stop  container_name/container_id<br> 

14) To restart a running container<br> 
   docker restart container_name/container_id<br> 
         To restart after 10 seconds<br> 
   docker restart  -t  10  container_name/container_id<br> 

15) To delete a stopped container<br> 
    docker  rm  container_name/container_id<br>

16) To delete a running container<br> 
    docker  rm  -f  container_name/container id<br> 

17) To stop all running containers<br> 
    docker stop $(docker ps -aq)<br> 

18) To restart all containers<br> 
    docker restart $(docker ps -aq)<br> 

19) To remove all stopped containers<br> 
    docker rm $(docker ps -aq)<br> 

20) To remove all contianers(running and stopped)<br> 
    docker rm -f  $(docker ps -aq)<br> 

21) To see the logs generated by a container<br> 
   docker logs container_name/container_id<br> 

22) To see the ports used by a container<br> 
   docker port container_name/container_id<br> 

23) To get detailed info about a container<br> 
   docker inspect container_name/container_id<br> 

24) To go into the shell of a running contianer which is moved into background<br> 
   docker attach container_name/container id<br> 

25) To execute anycommand in a container<br> 
   docker exec -it container_name/container_id command<br>
   Eg: To launch the bash shell in a contianer<br> 
   docker exec -it container_name/container_id    bash<br> 

26) To create a container from a docker image  ( imp )<br>
     docker run image_name<br>   


# Run command options in Docker

-it 	for opening an interactive terminal in a container<br> 

--name 	Used for giving a name to a container<br> 

-d 	Used for running the container in detached mode as a background process<br> 

-e 	Used for passing environment varaibles to the container<br> 

-p 	Used for port mapping between port of container with the dockerhost port.<br>

 -P 	Used for automatic port mapping ie, it will map the internal port of the container<br> 
                with some port on host machine.<br> 
               This host port will be some number greater than 30000<br> 

-v 	Used for attaching a volume to the container<br> 

--volume-from 	 Used for sharing volume between containers<br> 

--network 	Used to run the contianer on a specific network<br> 

--link 		Used for linking the container for creating a multi container architecture<br> 

--memory  	Used to specify the maximum amount of ram that the container can use<br> 

# Downloading the image and creating container:

To download tomcat image<br>
docker pull tomee<br>

To check downloaded images<br>
docker images<br>		

To create a container from an image<br>
docker run --name mytomcat  -p   7070:8080   tomee<br>
Note: If two containers are running at same port, inorder to avoid conflict while accessing the application we use "Port mapping". 


# Stopping the container and removing the container
To stop the container we use the command<br>
docker stop container_name<br>

To remove the container<br>
docker rm -f container_name

# Understanding detached mode and interactive mode
To create ubuntu container<br>
docker run --name myubuntu  -it ubuntu<br>

Observation:  -it stands for interavtive mode. You have automatically entered into ubuntu bash<br>

Scenario 1:<br>
Start tomcat as a container and name it as "webserver". Perform port mapping and run this container in detached mode<br>

docker run --name  webserver  -p 7070:8080  -d tomee<br>

To access homepage of the tomcat container<br>
Launch any browser:<br>
public_ip_of_dockerhost:7070<br>

----------------------------------------------------------------------------<br>
Scenario 2:<br>
Start jenkins as a container in detached mode , name is as "devserver", perform port mapping<br>

docker run -d  --name  devserver  -p 9090:8080 jenkins<br>

To access home page of jenkins ( In browser)<br>
public_ip_of_dockerhost:9090<br>

----------------------------------------------------------------------------<br>

Scenario 3:<br>
Start nginx as a container and name as "appserver", run this in detached mode ,   perform automatic port mapping<br> 

Generally we pull the image and run the image<br>

Instead of pulling, i directly used this command<br>  

docker run --name  appserver  -P  -d  nginx<br> 
( if image is not available, it perform pull operation automatically )<br>
( Capital P  , will perform automatic port mapping )<br>

-----------------------------------------------------------------------------<br>
To start mysql  as container, open interactive terminal in it, create a sample table.<br>

docker run  --name  mydb  -d  -e MYSQL_ROOT_PASSWORD=sunil  mysql:5<br>
To check<br>
docker container ls<br>

I want to open bash terminal of  mysql<br>
docker  exec   -it  mydb  bash<br>

To connect to mysql database<br>
mysql  -u  root  -p<br> 

# Multi container architecture using docker
This can be done in  2  ways<br>
1) --link<br>
2) docker-compose<br>

EXample 1: Start two busybox containers and create link between them<br>

Create 1st busy box container<br>
docker run --name c10 -it   busybox<br>
How to come out of the container without exit<br>
( ctrl + p  + q)<br>

Create 2nd busy box container  and establish link to c1 container<br>
docker run --name  c20 --link c10:c10-alias  -it  busybox   ( c10-alias  is  alias name)<br>

How to check  link is established for not?<br>
ping c10<br>

Ctrl +c  ( to come out from ping )<br>
(ctrl + p  + q)<br>

Example 2: Creating development environment using docker<br>
------------------------------------------------------------<br>
Start mysql as container and link it with wordpress container.<br>

Developer should be able to create wordpress website<br>


1) To start mysql as container<br>
docker run --name mydb  -d  -e  MYSQL_ROOT_PASSWORD=sunil  mysql:5<br>

( if container is already in use , remove it using docker rm -f  mydb)<br>

Check whether the container is running or not.<br>
docker container ls<br>

2) To start wordpress container<br>
docker run  --name mysite  -d  -p 5050:80 --link mydb:mysql  wordpress<br>

Check wordpress installed or not.<br>
Open browser<br> 
public_ip:5050<br>
18.138.58.3:5050<br>

# Create LAMP  Architecture using docker

L -- linux<br>
A -- apache tomcat<br>
M -- mysql<br>
P --  php<br>
(Linux os we already have)<br>
1) To start mysql as container<br>
docker run --name mydb  -d  -e  MYSQL_ROOT_PASSWORD=sunil  mysql:5<br>


2) To start tomcat as container<br>
docker run  --name  apache  -d  -p 6060:8080  --link mydb:mysql  tomcat<br>


To see the list of containers<br>
docker container ls<br>

To check if tomcat is linked with mysql<br>
docker inspect apache      ( apache is the name of the container)<br>


3) To start php as container<br>
docker  run --name php  -d --link apache:tomcat  --link mydb:mysql    php<br>


# Create CI-CD environment, where jenkins container is linked with two tomcat containers.<br>

Lets delete all the container<br>
docker rm  -f  $(docker ps -aq)<br>

To start jenkins as a container<br>
docker run  --name  devserver  -d -p 7070:8080 jenkins/jenkins<br>

To check jenkins is running or not?<br>
Open browser<br>
public_ip:7070<br>

We need two tomcat containers  ( qa server and prod server)<br>
docker run --name  qaserver  -d  -p 8080:8080 --link devserver:jenkins tomee<br>

To check the tomcat   use public_ip but port number will be 8080<br>
docker run --name  prodserver  -d  -p 9090:8080 --link devserver:jenkins tomcat<br>


# Creating testing environment using docker:<br>

Create selenium hub container, and link it with two node containers. One node with firefox installed, another node with chrome installed. Tester should be able to run selenuim automation programs for testing the application on multiple browsers.<br>

Search for selenium:<br>
We have a image -  selenium/hub<br>

To start selenium/hub as container<br>
docker run --name  hub  -d -p 4444:4444   selenium/hub<br>


In hub.docker.com<br>
we also have-  selenium/node-chrome-debug    ( It is ubuntu container with chrome)<br>

To start it as a container and link to hub ( previous container)<br>
docker run --name chrome  -d -p 5901:5900  --link hub:selenium   selenium/node-chrome-debug<br>

In hub.docker.com<br>
we also have-  selenium/node-firefox-debug<br>

To start it as a container and link to hub ( It is ubuntu container with firefox)<br>
docker run --name firefox  -d -p 5902:5900  --link hub:selenium   selenium/node-firefox-debug<br>

To see the list of container<br>
docker container ls<br>

Note: firefox and chrome containers are GUI containers.<br>
To see the GUI interface to chrome / firefox container<br>

Download and install vnc viewer<br>
In VNC viewer search bar<br>
public_ip_dockerhost:5901<br>

Password - secret<br>

------------------------------------------------------------------------------------<br>


All the commands we learnt till date are adhoc commands.<br>

In the previous usecase we have installed two containers ( chrome and firefox)<br>
Lets say you need 80 containers?<br>
Do we need to run 80 commands?<br>
 
Instead of 80 commands, we can use docker compose<br>


# Docker compose

This is a feature of docker using which we can create multicontainer architecture using yaml files. This yaml file contains information about the  containers that we want to launch and how they have to be linked with each other.Yaml is a file format. It is not a scripting language.
Yaml will store the data in key value pairs<br>
Lefthand side - Key<br>
Righthand side - Value<br>
Yaml file is space indented.<br>

Sample Yaml file<br>
-------------------------<br>

bahadoorsoft:<br>
 trainers:<br>
  bahadoor: Devops<br>
  daya: Python<br>
 Coordinators:<br>
  sai: Devops<br>
  bahadoor: AWS<br>
...<br>


bahadoorsoft -- root  element<br>

To validate the abvove Yaml file<br>
Open  http://www.yamllint.com/<br>
Paste the above code  -- Go button<br>

------------------------------------------------------------------------------------------------------<br>

# Installing Docker compose

1) Open https://docs.docker.com/compose/install/<br>
2) Go to linux section<br>
Copy and pase the below two commands<br>

sudo curl -L "https://github.com/docker/compose/releases/download/1.24.0/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose<br>
sudo chmod +x /usr/local/bin/docker-compose<br>
	
How to check docker compose is installed or not?<br>

docker-compose  --version<br>


# Create a docker compose file for setting up dev environment.

mysql container is linked with wordpress container.<br>
vim docker-compose.yml  (Name of the file should be docker-compose.yml)<br>

version: '3'<br>

services:<br>
 mydb:<br><br>
  image: mysql:5<br>
  environment:<br>
   MYSQL_ROOT_PASSWORD: bahadoor<br>

 mysite:<br>
  image: wordpress<br>
  ports:<br>
   - 5050:80<br>
  links:<br>
   - mydb:mysql<br>
...<br>


Lets remove all the running container<br>
docker rm -f $(docker ps -aq)<br>

How to start the above services from dockerfile<br>
docker-compose  up<br>

We got lot of logs coming on the screen. to avoid it we use -d  option<br>  

docker-compose stop<br>

Remove the container
docker rm -f $(docker ps -aq)<br>
 
docker-compose up  -d<br>

To check wordpress<br>
public_ip:5050<br>
To stop both the containers<br>
docker-compose stop<br>

-------------------------------------------------------------------------------------------------------------------<br>


# Create a docker compose file for setting up LAMP architecture 

vim docker-compose.yml<br>

---<br>
version: '3'<br>

services:<br>
 mydb:<br>
  image: mysql:5<br>
  environment:<br>
   MYSQL_ROOT_PASSWORD: bahadoor<br>

 apache:<br>
  image: tomee<br>
  ports:<br>
   - 6060:8080<br>
  links:<br>
   - mydb:mysql<br>


 php:<br>
  image: php<br>
  links:<br>
   - mydb:mysql<br>
   - apache:tomcat<br>
...<br>

docker-compose up -d<br>

To see the list of the containers<br>
docker container ls<br>
(Observation - we are unable to see the php container)<br>
docker ps -a<br>

--------------------------------------------------------------------------------------------------------------<br>
Ex: Docker-compose file for setting up CI-CD Environment.<br>
jenkins container is linked with two tomcat containers<br> 

vim docker-compose.yml<br>

---<br>
version: '3'<br>
services:<br>
 devserver:<br>
  image: jenkins/jenkins<br>
  ports:<br>
   - 7070:8080<br>

 qaserver:<br>
  image: tomee<br>
  ports:<br>
   - 8899:8080<br>
  links:<br>
   - devserver:jenkins<br>

--------------------------------------------------------------------------------------------------------<br>


 prodserver:<br>
  image: tomee<br>
  ports:<br>
   - 9090:8080<br>
  links:<br>
   - devserver:jenkins<br>
...<br>
------------------------------------------------------------------------------------------------------------<br>
docker rm -f $(docker ps -aq)<br>
docker-compose up -d<br>
docker container ls<br>

To check:<br>
public_ip:7070  (To check jenkins)<br>
public_ip:8899 (Tomcat  qa server)<br>
public_ip:9090 (Tomcat  prod server)<br>


Docker-compose file to set up testing environment.<br>
selenium  hub container is linked with two node containers.<br>


vim docker-compose.yml<br>

---<br>
version: '3'<br>
services:<br>
 hub:<br>
  image: selenium/hub<br>
  ports:<br>
   - 4444:4444<br>

 chrome:<br>
  image: selenium/node-chrome-debug<br>
  ports:<br>
   - 5901:5900<br>
  links:<br>
   - hub:selenium<br>

 firefox:<br>
  image: selenium/node-firefox-debug<br>
  ports:<br>
   - 5902:5900<br>
  links:<br>
   - hub:selenium<br>
...<br>
--------------------------------------------------------------------------------------------------<br>
docker-compose up -d<br>

docker container ls<br>

As it is GUI container, we can access using VNC viewer<br>
Open VNC viewer<br>
52.77.219.115:5901<br>
password: secret<br>


---------------------------------------------------------------------------------------------------<br>

# Docker volumes

Docker containers are  ephemeral (temporary). Where as the data processed by the container should be permanent. Generally, when a container is deleted all its data will be lost.
To preserve the data, even after deleting the container, we use volumes.

Volumes are of two types<br>
1) Simple docker volumes<br>
2) Docker volume containers ( Sharable volume)<br>




Simple docker volumes<br>
---------------------
These volumes are used only when we want to access the data, even after the container is deleted. But this data cannot be shared with other containers.



Usecase<br>
1) Create a directory called /data ,<br>  
start centos as container and mount /data as volume.<br> 
Create files in mounted volume in centos container,<br>
exit from the container and delete the container. Check if the files are still available.<br>




Lets create a folder  with the name<br>  
mkdir  /data<br>

docker run --name c1 -it -v /data centos  (v option is used to attach volume)<br>

ls  ( Now, we can see the data folder also in the container)<br>

cd data<br>
touch file1   file2<br>
ls<br>
exit  (To come out of the container)<br>
docker inspect c1<br>

We can see under mounts "data" folder it located in the host machine.<br>
Copy the path<br>


/var/lib/docker/volumes/c5c85f87fdc3b46b57bb15f2473786fe7d49250227d1e9dc537bc594db001fc6/_data<br>

Now, lets delete te container<br>
docker rm -f c1<br>

After deleting the container, lets go to the location of the data folder<br>

cd /var/lib/docker/volumes/d867766f70722eaf8cba651bc1d64c60e9f49c5b1f1ebb9e781260f777f3c7e8/_data<br>
ls  ( we can see file1  file2 )<br>

(Observe , the container is deleted but still  the data is persistant )<br>

--------------------------------------------------------------------------------------------------------------------<br>

# Docker volume containers

These are also known as reusable volume. The volume used by one container can be shared with other containers. Even if all the containers are deleted, data will still be available on the docker host.

Ex:<br>
sudo su -<br>

Lets create a directory      /data<br>
mkdir  /data<br> 

Lets Start  centos as container<br>
docker run --name  c1  -it  -v /data centos<br> 
ls  ( we can see the list of files and dir in centos )<br>


cd data<br>
ls  ( currently we have no files )<br>

Lets create  some files<br>
touch file1  file2  ( These two files are available in c1 container)<br>

Comeout of the container without exit<br>
Ctrl +p  Ctrl +q  ( container will still runs in background )<br>


Lets Start another  centos as container ( c2 container should use the same volume as c1)<br>
docker run --name  c2  -it  --volumes-from c1  centos<br> 



cd data<br>
ls  ( we can see the files created by c1 )<br>

Lets create some more files<br>
touch file3  file4<br>
ls  ( we see 4 files )<br>

Comeout of the container without exit<br>
Ctrl +p  Ctrl +q  ( container will still runs in background )<br>

Lets Start another  centos as container<br>
docker run --name  c3  -it  --volumes-from c2 centos<br> 



























Using docker file:<br>
This is a simple text file, which uses  predefinied keywords for creating customized docker images.<br>
Key words used in docker file  ( case sensitive )<br>

1) FROM  --  used to specify the base image from which the docker file has to be created.<br>

2) MAINTAINER -- This represents name of the organization or the author who created this docker file.<br>
 
3) CMD   -- This is used to specify the initial command that should be executed when the container starts.<br>

4) ENTRYPOINT - used to specify the default process that should be executed when container starts.
It can also be used for accepting arguments from the CMD instruction.<br>

5) RUN  -- Used for running linux commands within the container. It is generally helpful for installing the software in the container.<br>

6) USER  --      used to specify the default user who should login into the container.<br>

7) WORKDIR --   Used to specify default working directory in the container<br>

8) COPY  --  Copying the files from the host machine to the container.<br>

9) ADD  -- Used for copying files  from host to container, it can also be used for downloading files from remote servers.<br>

10) ENV  --  used for specifying the environment variables that should be passed to the container.<br>

EXPOSE -- Used to specify the internal port of the container<br>
VOLUME  --  used to specify the default volume that should be attached to the container.<br>
LABEL  --  used for giving label to the container<br>
STOPSIGNAL  -- Used to specify the key sequences that have to be passed in order to stop the container.<br>

--------------------------------------------------------------------------------------------------------------------------<br>


Create a dockerfile by taking nginx as the base image and specify the maintainer as logiclabs. Construct an image from the dockerfile.

Creating customized docker images by using docker file.<br>

$ sudo su -<br>
vim dockerfile<br>

FROM nginx<br>
MAINTAINER logiclabs<br>



TO build an image from the dockerfile<br>
docker build -t  mynginx .<br>

 (t stands for tag, .  stands for current working dir mynginx is the new image name )<br>


When ever i start my container, i want a program to get executed.<br>

vim dockerfile<br>

FROM centos<br>
MAINTAINER logiclabs<br>
CMD ["date"]<br>


To build an image from the dockerfile<br>
docker build -t  mycentos  . <br>

TO see the image<br>
docker images<br>

Running conainer from the image<br>
docker run -it   mycentos<br>



In one docker file, we can have one CMD instruction. If we give two  CMD instruction, it executes the latest one
Lets try<br>



vim dockerfile<br>
FROM centos<br>
MAINTAINER logiclabs<br>
CMD ["date"]<br>
CMD ["ls", "-la"]<br>

docker build -t  mycentos . <br>

docker run -it   mycentos<br>
( Observation, we get ls -la output )<br>

---------------------------------------------------------------------------------------------------------------<br>
In ubuntu container, I want to install git in it.<br>


Lets remove the docker file<br>
rm dockerfile<br>
#  vim dockerfile<br>

FROM ubuntu<br>
MAINTAINER logiclabs<br>
RUN apt-get update<br>
RUN apt-get install -y git<br>


Note:  CMD  -- will run  when container starts.<br>
       RUN  --  will executed when image is created.<br>


docker build -t  myubuntu . <br>

Lets see the images list and space consumed by  our  image<br>
docker images<br>

docker run -it   myubuntu<br>
 git  --version<br>
 exit<br>


---------------------------------------------------------------------------------------------------------<br>

Lets perform version controlling  in docker file<br>



mkdir  docker <br>
mv dockerfile  docker<br>
cd docker<br>
ls<br>


docker# git init<br>
docker# git status <br> 
docker# git add .<br>

docker# git commit  -m "a"<br>

( we get error we need to config git)<br>
docker# git config --global user.name "bahadoor009"<br>
docker# git config --global user.email "bahadoor009@gmail.com"<br>

Now, run the above commit command  ( git commit )<br>

docker# vim dockerfile  ( lets make some changes add another RUN command )<br>

FROM ubuntu<br>
MAINTAINER logiclabs<br>

RUN apt-get update<br>
RUN apt-get install -y git<br>
RUN apt-get install -y default-jdk<br>

:wq

docker# git add .<br>
docker# git commit  -m "b"<br>

Now lets see the docker file<br>
vim dockerfile  ( we see the latest one )<br>

Now, I want to have previous version<br>
git log  --oneline  (  to see the list of all the commits)<br>

We want to move to "a" commit  ( take note of commit id )<br>

git reset --hard  10841c3<br>




Now lets see the docker file<br>
vim dockerfile  ( we see the old one )<br>

---------------------------------------------------------------------------------------------------



# Cache busting

Whenever an image is build from a dockerfile, docker reads its memory and checks which instructions were already executed. 
These steps will not be reexecuted. 
It will execute only the latest instructions. This is a time saving mechanism provided by docker. 

But, the disadvantage is, we can end up installing software packages  from a repository which is updated long time back. 


Ex:<br>

cd docker<br>
vim dockerfile<br>

Lets just add one more instruction<br>

FROM ubuntu<br>
MAINTAINER logiclabs<br>

RUN apt-get update<br>
RUN apt-get install -y git<br>
RUN apt-get install -y tree<br>                 




Lets build an image<br>
# docker build -t myubuntu  .<br>


( Observe the output,  Step 2, 3, 4 is using cache.  Only step 5 is executed freshly )<br>

Advantage: time saving mechanism<br>

Disadvantage : Lets say, you are running after 4 months, We are installing tree from apt which is updated long time back. )<br>


To avoid this disadvanatge we use cache busting<br>

Note: cache busting is implemented using && symbol.<br>
Which ever statement in the docker file has &&  will be re-executed. <br>

vim dockerfile<br>

FROM ubuntu<br>
MAINTAINER logiclabs<br>

RUN apt-get update && apt-get install -y git tree<br>


Lets build an image<br>
docker build -t myubuntu .<br>

( Observe the output, step 3  - It is not using cache )




cd data<br>
ls  ( we can see 4 files )<br>
touch file5  file6<br>
ls<br>

Comeout of the container without exit<br>
Ctrl +p  Ctrl +q  ( container will still runs in background )<br>

Now, lets connect to any container which is running in the background<br>
docker attach  c1<br>
ls  ( you can see all the files )<br>
exit<br>

Identify the mount location<br>
$ docker inspect  c1<br>
( search for the mount section )<br>

Take a note of the source path<br>

/var/lib/docker/volumes/e22a9b39372615727b964151b6c8108d6c02b13114a3fcce255df0cee7609e15/_data<br>



Lets remove all the container<br>
docker rm -f  c1  c2  c3<br>

Lets go to the source path<br>
cd /var/lib/docker/volumes/e22a9b39372615727b964151b6c8108d6c02b13114a3fcce255df0cee7609e15/_data<br>
ls  ( we can see all the files )<br>


# Creating customized docker images


Whenever docker container is deleted, all the softwares that we have installed within the container will also be deleted.<br>



If we can save the container as an image, then we can preserve the softwares.<br>


This creation of customized docker images can be done in two ways.<br>
1) using docker commit command<br>
2) using docker file<br>


Using docker commit<br>
docker run  --name c1 -it  ubuntu<br>

Update apt repository<br>

apt-get update<br>
apt-get install git<br>
 
To check the git<br>
git  --version<br>
exit<br>

To save the container as image (snapshot)<br>
docker commit   c1   myubuntu<br>

To see the list of images<br>
docker images  ( you can see the image which you have created )<br>

Now lets run the image which we have created<br>
docker run --name c2 -it   myubuntu<br>
git --version  ( git is pre installed )<br>





















Using docker file<br>

This is a simple text file, which uses  predefinied keywords for creating customized docker images.<br>

Key words used in docker file  ( case sensitive )<br>

1) FROM  --  used to specify the base image from which the docker file has to be created.<br>

2) MAINTAINER -- This represents name of the organization or the author who created this docker file.<br>

3) CMD   -- This is used to specify the initial command that should be executed when the container starts.<br>

4) ENTRYPOINT - used to specify the default process that should be executed when container starts. It can also be used for accepting arguments from the CMD instruction.<br>

5) RUN  -- Used for running linux commands within the container. It is generally helpful for installing the software in the container.<br>

6) USER  --      used to specify the default user who should login into the container.<br>

7) WORKDIR --  Used to specify default working directory in the container<br>

8) COPY  --  Copying the files from the host machine to the container.<br>

9) ADD  -- Used for copying files  from host to container, it can also be used for downloading files from remote servers.<br>

10) ENV  --  used for specifying the environment variables that should be passed to the container.<br>

EXPOSE -- Used to specify the internal port of the container<br>
VOLUME  --  used to specify the default volume that should be attached to the container.<br>
LABEL  --  used for giving label to the container<br>
STOPSIGNAL  -- Used to specify the key sequences that have to be passed in order to stop the container.<br>


Create a dockerfile by taking nginx as the base image and specify the maintainer as logiclabs. Construct an image from the dockerfile.<br>

Creating customized docker images by using docker file.<br>

$ sudo su -<br>
vim dockerfile<br>

FROM nginx<br>
MAINTAINER logiclabs<br>

TO build an image from the dockerfile<br>
docker build -t  mynginx .<br> 

( t stands for tag, .  stands for current working dir mynginx is the new image name    )<br>




To see the image<br>
docker images<br>

------------------------------------------------------------------------------------------------------------------


When ever i start my container, i want a program to get executed.<br>
vim dockerfile<br>

FROM centos<br>
MAINTAINER logiclabs<br>
CMD ["date"]<br>


TO build an image from the dockerfile<br>
docker build -t  mycentos  .<br> 

To see the image<br>
docker images<br>

Running conainer from the image<br>
docker run -it   mycentos<br>

-----------------------------------------------------------------------------------------------------------------------<br>

In one docker file, we can have one CMD instruction.<br>
If we give two  CMD instruction, it executes the latest one<br>
Lets try<br>



vim dockerfile<br>
FROM centos<br>
MAINTAINER logiclabs<br>
CMD ["date"]<br>
CMD ["ls", "-la"]<br>

:wq<br>

docker build -t  mycentos .<br> 

docker run -it   mycentos<br>
( Observation, we get ls -la output )<br>


-------------------------------------------------------------------------------------------------------------------------<br>
In ubuntu container, I want to install git in it.<br>


Lets remove the docker file<br>
rm dockerfile<br>
vim dockerfile<br>

FROM ubuntu<br>
MAINTAINER logiclabs<br>
RUN apt-get update<br>
RUN apt-get install -y git<br>

:wq<br>


Note:  CMD  -- will run  when container starts.<br>
       RUN  --  will executed when image is created.<br>


docker build -t  myubuntu .<br> 

Lets see the images list and space consumed by  our  image<br>
docker images<br>
docker run -it   myubuntu<br>
git  --version<br>
exit<br>


--------------------------------------------------------------------------------------------------------------------------<br>

Lets perform version controlling  in docker file<br>



mkdir  docker <br>
mv dockerfile  docker<br>
cd docker<br>
ls<br>


docker# git init<br>
docker# git status<br>  
docker# git add .<br>

docker# git commit  -m "a"<br>

( we get error we need to config git)<br>
docker# git config --global user.name "sunildevops77"<br>
docker# git config --global user.email "sunildevops77@gmail.com"<br>

Now, run the above commit command  ( git commit )<br>

docker# vim dockerfile  ( lets make some changes add another RUN command )<br>

FROM ubuntu<br>
MAINTAINER logiclabs<br>

RUN apt-get update<br>
RUN apt-get install -y git<br>
RUN apt-get install -y default-jdk<br>

:wq<br>

docker# git add .<br>
docker# git commit  -m "b"<br>

Now lets see the docker file<br>
vim dockerfile  ( we see the latest one )<br>

Now, I want to have previous version<br>
git log  --oneline  (  to see the list of all the commits)<br>

We want to move to "a" commit  ( take note of commit id )<br>

git reset --hard  10841c3<br>




Now lets see the docker file<br>
vim dockerfile  ( we see the old one )<br>

----------------------------------------------------------------------------------------------------------------------<br>



# Cache busting

Whenever an image is build from a dockerfile, docker reads its memory and checks which instructions were already executed. <br>
These steps will not be reexecuted. 
It will execute only the latest instructions. This is a time saving mechanism provided by docker. <br>

But, the disadvantage is, we can end up installing software packages  from a repository which is updated long time back. <br>


Ex:<br>

cd docker<br>
vim dockerfile<br>

Lets just add one more instruction<br>

FROM ubuntu<br>
MAINTAINER logiclabs<br>

RUN apt-get update<br>
RUN apt-get install -y git<br>
RUN apt-get install -y tree <br>                


:wq<br>


Lets build an image<br>
docker build -t myubuntu  .<br>


( Observe the output,  Step 2, 3, 4 is using cache.  Only step 5 is executed freshly )<br>

Advantage: time saving mechanism<br>



Disadvantage : Lets say, you are running after 4 months, We are installing tree from apt which is updated long time back. )<br>


To avoid this disadvanatge we use cache busting:<br>

Note: cache busting is implemented using && symbol.<br>
Which ever statement in the docker file has &&  will be re-executed.<br> 

vim dockerfile<br>

FROM ubuntu<br>
MAINTAINER logiclabs<br>

RUN apt-get update && apt-get install -y git tree<br>

:wq<br>

Lets build an image<br>
docker build -t myubuntu .<br>

( Observe the output, step 3  - It is not using cache )<br>













Ex:  Create a dockerfile, for using ubuntu as base image, and install java in it.<br>
Download jenkins.war  and make execution of "java -jar jenkins.war" as the default process.<br>

Every docker image come with default process.<br>
As long as default process is running, the container will be running condition.<br>

The moment, the default process is closed, the container will be exited.<br>

Lets remove all the container<br>
docker rm -f $(docker ps -aq)<br>
 

Observation 1:<br>
When we start ubuntu container, we use below command<br>
docker run --name  c1  -it  ubuntu<br>

To comeout of the container we use  Ctrl + p + q<br>

docker container ls<br>
( our container c1  is running in the background )<br>

Observation 2:<br>
When we start jenkins container, we use below command<br>
docker run --name j1  -d  -P    jenkins/jenkins<br>


Now, I want to open interactive terminal to enter jenkins<br>
docker  exec -it  j1  bash<br>


( In ubuntu container, I can directly go into -it terminal, where as in jenkins i am running an additional command  exec ? )<br>


Lets try to go to interactive  terminal  in docker run command )<br>
docker run  --name  j2  -it  jenkins/jenkins<br>
( we are not getting interactive terminal )<br>



I want to run tomcat as container<br>
docker run  --name   t1  -d   -P   tomee<br>


Lets find the reason<br>

docker container ls  ( to see the list of containers )<br>

Observer the command section.<br>
It tells you the default process that gets executed, when we start the container.<br>

Container			Default process <br>
tomcat			catalina.sh<br>
jenkins			/bin/tini<br>
ubuntu			/bin/bash<br>

bash -- is nothing but the terminal.<br>

For linux based container, the default process is shell process <br>
( ex of shell process are  bash shell, bourne shell etc )<br>
Hence we are able to enter -it mode  in ubuntu )<br>



We are trying to change the default process of the container.<br>
-------------------------------------------------------------<br>

vim dockerfile<br>

FROM ubuntu<br>
MAINTAINER logiclabs<br>

RUN apt-get update<br>
RUN apt-get install -y default-jdk<br>

ADD http://mirrors.jenkins.io/war-stable/latest/jenkins.war  /<br>
ENTRYPOINT ["java","-jar","jenkins.war"]<br>

:wq<br>

Build an image from the dockerfile<br>
docker build -t  myubuntu  .<br>

To see the list of images  ( we can see our new image )<br>
docker image ls<br>

To start container from new image<br>
docker run  myubuntu     ( Observe the logs generated on the screen, we got logs related to jenkins , jenkins is fully up and running )<br>

Its an ubuntu container, it is behaving as a jenkins container )<br>

Ctrl +c<br>

RUn the below command<br>
docker ps -a<br>

For myubuntu   the command is  java -jar  jenkins.war<br>
For  ubuntu    the commans is     /bin/bash<br>



---------------------------------------------------------------------------------------------------------
Working on docker registry<br>
Registry is a location where docker images are saved.<br>
Types of registry<br>
1) public registry<br>
2) private registry<br>

public registry is hub.docker.com<br>
Images uploaded here are available for everyone.<br>


Usecase: Create a customized ubuntu image, by installing  tree in it.<br>

Save this container as an image, and upload this image in docker hub.<br>

Step 1: Create a new account in hub.docker.com<br>

Step 2: Creating our own container<br>
docker run --name  c5 -it  ubuntu<br>

Lets install tree package in this container<br>
apt-get update<br>
apt-get  install tree<br>
exit<br>

Step 3: Save the above container as an image<br>
docker commit  c5  sunildevops77/ubuntu_img291<br>  
( sunildevops77/ubuntu_img291  -- is the image name )<br>

Note: Image name should start with docker_id/<br>

To see the list of images<br>
docker image ls  ( we can see the new image )<br>

To upload the image to hub.docker.com  ( docker login command is used )<br>

docker login   ( provide docker_id and password )<br>

To upload the image<br>
docker push  <image_name><br>
docker push sunildevops77/ubuntu_img291<br>

login to docker hub to see your image<br>

---------------------------------------------------------------------------------------------------------

# Container orchestration

This is the process of running docker containers in a distributed environment, on multiple docker host machines.<br>
All these containers can have a single service running on them and they share the resources between eachother, even running on different host machines.<br>

Docker swarm is the tool used for performing container orchestration<br>


Advantages<br>

1) Load balancing<br>
2) scaling of containers<br>
3) performing rolling updates<br>
4) handling failover scenarios<br>

 
----------------------------------------------------------------------------------------------------------

Machine on which docker swarm is installed is called as manager.<br>
Other machines are called as workers.<br>


Lets create 3 machines<br>
Name is as Manager, Worker1, Worker2<br>

All the above machines should have docker installed in it.<br>
Install docker using get.docker.com<br>

( Optional step to change the  prompt )<br>
After installing docker in the 1st machine ( Manager ),  Lets change the host name.<br>
Host name will be available in the file hostname. We will change the hostname to manager.<br>

vim /etc/hostname<br>
Manager<br>

:wq<br>

After changing the hostname, lets restart the machine<br>
init 6<br>


Similary repeat the same in worker1 and worker2<br>

Connect to Manager, install docker swarm in it.<br>

$ sudo su -<br>

Command to install docker swarm  in manager machine<br>

docker swarm init --advertise-addr  private_ip_of_manager<br>
docker swarm init --advertise-addr  172.31.27.151<br>

Please read the log messages<br>

Now, we need to add workers to manager<br>
Copy the  docker swarm join command in the log and run in the worker1  and worker2<br>

Open another gitbash terminal, connect to worker1<br>

sudo su -<br>

docker swarm join --token SWMTKN-1-0etsmfa26vreeytq278q8ohhi73il7j1lpnrzzlowuld1r8yex-9x04pjmiq85jxjzjayzlglh1c 172.31.27.151:2377<br>

Repeat for worker2<br>


To see the no of nodes from the manager<br>

Manager # docker  node ls   ( we can see manager, worker1  and worker 2)<br>

















Load balancing:<br>
Each docker container is designed to withstand a specific  user load.<br>
When the load increases, we can replica containers in docker swarm and distribute the load.<br>

Ex: Start tomcat in docker swarm with 5 replicas and name it as webserver.<br>

Manager# docker service create --name webserver -p 9090:8080 --replicas 5  tomee<br>

( 5 conainers with the same service, distributed load in 3 machines)<br>


How to see where thay are running?<br>
Manager# docker service  ps  webserver<br>

Lets take the note<br>
Manager - 1 container<br>
Worker1 - 2 container<br>
Worker2 - 2 container<br>

--------------------------------------------------------------------------------------------------------------------------
Note: Only one tomcat is running and load is shrared to 3 machines<br>

Lets check<br>
public_ip_manager:9090  ( Will show tomcat page )<br>
public_ip_worker1:9090  ( Will show tomcat page )<br>
public_ip_worker2:9090  ( Will show tomcat page )<br>


Ex 2:  Start mysql in docker swarm with 3 replicas.<br>

Manager# docker service create --name mydb --replicas 3   -e MYSQL_ROOT_PASSWORD=sunil mysql:5<br>

How to see where thay are running?<br>
Manager# docker service  ps  mydb<br>

To know the total no of services running in docker swarm<br>
Manager# docker service ls<br>

If you delete a container, it will create another container.<br>

Now,<br>
Manager# docker service  ps  mydb<br>

We can see one container is running in  Manager machine<br>
I want to delete the container which is running in manager<br>

Manager# docker container ls<br>
( we can see 1 mysql container, 1 tomcat container )<br>

Take note of the container_id  of mysql<br>
67238f47bc60<br>

To delete the container<br>
docker rm -f   67238f47bc60<br>



Now lets check the mydb service<br>
docker service  ps  mydb ( we can see one service is failed, automatically 2nd service is started)<br>
At anypoint of time, 3 container will be running.<br>

-----------------------------------------------------------------------------------------------------------------------------------

Scaling of containers: <br>

When business requirement increases, we should be able to increase the no of replicas.<br>
Similarly, we should also be able to decrease the replica count based on business requirement. This scaling should be done without any downtime.<br>
 
Ex 3:  Start nginx with 5 replicas, later scale the services to 10.<br>


docker service  create  --name appserver -p 8080:80  --replicas 5 nginx<br>

docker service ps appserver<br>

Command to scale<br>
docker service scale  appserver=10<br>

To check<br>
docker service ps appserver<br>

Now I want only two containers<br>
docker service scale  appserver=2<br>

To check<br>
docker service ps appserver<br>


To remove a node from the docker swarm<br>
Two ways<br>
1) Manager can drain<br>
2) Node can leave<br>




To see the list of nodes<br>
docker node ls<br>

docker node update --availability drain  Worker1<br>

All the container running in Worker1 , will be migrated to Worker2 or manager.<br>

docker service ps mydb<br>
docker node ls<br>

To add the node<br>
docker node update --availability active  Worker1<br>

docker node ls<br>


2nd Way  ( Node can leave )<br>

Lets Connect to worker2 from git bash<br>

Worker2# docker swarm leave<br>

------------------------------------------------------------------------------------------------------<br>

To see the list of services<br>
docker service ls<br>

To delete the services<br>
Manager# docker service rm appserver mydb webserver<br>

Rolling Updates<br>

The services running in docker swarm, can be updated to any other version<br>
without any downtime. 
This is perfomed by docker swarm by updating one replica after another. This is called as rolling update.<br>

Ex: Create redis 3 service with 6 replicas. Update from redis 3 to redis 4 version.<br>

docker service create --name myredis --replicas 6 redis:3<br>

To check the replicas<br>
docker service ps myredis<br>

To update<br>
docker service update --image redis:4 myredis<br>

docker service ps myredis<br> 

I want to display running containers not shutdown containers<br>

docker service ps myredis | grep Shutdown  ( We get shutdown container )<br>
docker service ps myredis | grep -v Shutdown ( -v used for inverse operation )<br>

Performing rolling rollback , to downgrade to redis:3 version<br>

docker service update --rollback myredis<br>

To check redis:3 is running with 6 replicas and other version are shutdown.<br>
docker service ps myredis<br>
	

To add new nodes, in future, we need to docker swarm join command.<br>
To generate the command<br>
docker swarm  join-token  worker  ( We will get the command )<br>

 docker swarm join --token SWMTKN-1-0etsmfa26vreeytq278q8ohhi73il7j1lpnrzzlowuld1r8yex-9x04pjmiq85jxjzjayzlglh1c 172.31.27.151:2377<br>


To add a new machine as a manager<br>

docker swarm  join-token  manager<br>

docker swarm join --token SWMTKN-1-5wbamgr8x7gxabwtlm1j1i91bm5ilzotgna6bc0edubtwtjxi1-3jmzi67qdn5aawvielkcng2e4 172.31.34.112:2377<br>

If there are two managers, one will be leader<br>

docker node ls   ( we can see who is the leader )<br>
Decision of which is machine should be leader is automatic.<br>

If one manager goes down, other manager automatically become leader.<br>

To promote worker1 as a manager node<br>
docker node promote Worker1<br>

To demote Worker1 and make him back as a worker<br>
docker node demote Worker1<br>


### Docker networking
Docker networking is primarily used to establish communication between Docker containers and the outside world via the host machine where the Docker daemon is running.<br>
When we install docker, docker installs a network by default in the host machine called docker0<br>
<b>Docker network Drivers : </b> Docker handles communication between containers by creating a default bridge network, so you often donâ€™t have to deal with networking and can instead focus on creating and running containers. This default bridge network works in most cases.<br>
<b>The Bridge Driver</b><br>
This is the default. Whenever you start Docker, a bridge network gets created and all newly started containers will connect automatically to the default bridge network.<br>
<b>The Host Driver</b><br>
As the name suggests, host drivers use the networking provided by the host machine. And it removes network isolation between the container and the host machine where Docker is running. For example, If you run a container that binds to port 80 and uses host networking, the containerâ€™s application is available on port 80 on the hostâ€™s IP address. You can use the host network if you donâ€™t want to rely on Dockerâ€™s networking but instead rely on the host machine networking.<br>
<b>The None Driver</b><br>
The none network driver does not attach containers to any network. Containers do not access the external network or communicate with other containers. You can use it when you want to disable the networking on a container.<br>
<b>The Overlay Driver </b><br>
The Overlay driver is for multi-host network communication, as with Docker Swarm or Kubernetes. It allows containers across the host to communicate with each other without worrying about the setup. Think of an overlay network as a distributed virtualized network thatâ€™s built on top of an existing computer network.<br>
<b>Basic Docker Networking Commands</b><br>
connect     Connect a container to a network
  create      Create a network <br>
  disconnect  Disconnect a container from a network<br>
  inspect     Display detailed information on one or more networks<br>
  ls          List networks<br>
  prune       Remove all unused networks<br>
  rm          Remove one or more networks<br>


# -------------------BINGO..!!---------------------














